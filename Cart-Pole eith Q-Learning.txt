import gym
import time, math, random
from sklearn.preprocessing import KBinsDiscretizer
import numpy as np
from typing import Tuple
env = gym.make('CartPole-v1')

## Visualize Environment
#policy = lambda obs : 1
policy = lambda _,__,___, tip_velocity : int( tip_velocity > 0 )
for _ in range(3):
    obs = env.reset()
    for _ in range(80):
        actions = policy(*obs)
        obs, reward, done, info = env.step(actions)
        env.render()
        time.sleep(0.5)
env.close()

?env.env

obs

n_bins = ( 6 , 12 )   ## Dividing State Space in Descrete Buckets to convert states from continuous to discrete values
lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]
upper_bounds = [ env.observation_space.high[2], math.radians(50) ]

def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:
    """Convert continues state intro a discrete state"""
    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')
    est.fit([lower_bounds, upper_bounds ])
    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))

## Initialize Q Value Table with zeros
Q_table = np.zeros(n_bins + (env.action_space.n,))
Q_table.shape
 
## Create the Policy Function to select highest greedly Q-Value
def policy( state : tuple ):
    """Choosing action based on epsilon-greedy policy"""
    return np.argmax(Q_table[state])

## Update Q_Vales in table to optimizing goal
def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:
    """Temperal diffrence for updating Q-value of state-action pair"""
    future_optimal_value = np.max(Q_table[new_state])
    learned_value = reward + discount_factor * future_optimal_value
    return learned_value

## we want our agent to learn immediate at start and slow after some time. That's why we are making this function
def learning_rate(n : int , min_rate=0.01 ) -> float  :
    """Decaying learning rate"""
    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))

## Decaying Exploration Rate
def exploration_rate(n : int, min_rate= 0.1 ) -> float :
    """Decaying exploration rate"""
    return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / 25)))

## Here is our Training Starts
n_episodes = 200 
for e in range(n_episodes):
    
    # discretize state into buckets
    current_state, done = discretizer(*env.reset()), False
    
    while done==False:
        
        # policy action 
        action = policy(current_state) # exploit
        
        # insert random action
        if np.random.random() < exploration_rate(e) : 
            action = env.action_space.sample() # explore 
         
        # increment enviroment
        obs, reward, done, _ = env.step(action)
        new_state = discretizer(*obs)
        
        # Update Q-Table
        lr = learning_rate(e)
        learnt_value = new_Q_value(reward , new_state )
        old_value = Q_table[current_state][action]
        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value
        
        current_state = new_state
        
        # Render the cartpole environment
        env.render()